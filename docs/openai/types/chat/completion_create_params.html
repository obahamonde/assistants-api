<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>openai.types.chat.completion_create_params &mdash; OpenAI 1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=eafc0fe6" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=29a6c3e3"></script>
        <script src="../../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            OpenAI
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"></div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">OpenAI</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">openai.types.chat.completion_create_params</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for openai.types.chat.completion_create_params</h1><div class="highlight"><pre>
<span></span><span class="c1"># File generated from our OpenAPI spec by Stainless.</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">typing_extensions</span> <span class="kn">import</span> <span class="n">Literal</span><span class="p">,</span> <span class="n">Required</span><span class="p">,</span> <span class="n">TypedDict</span>

<span class="kn">from</span> <span class="nn">...types</span> <span class="kn">import</span> <span class="n">shared_params</span>
<span class="kn">from</span> <span class="nn">.chat_completion_tool_param</span> <span class="kn">import</span> <span class="n">ChatCompletionToolParam</span>
<span class="kn">from</span> <span class="nn">.chat_completion_message_param</span> <span class="kn">import</span> <span class="n">ChatCompletionMessageParam</span>
<span class="kn">from</span> <span class="nn">.chat_completion_tool_choice_option_param</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatCompletionToolChoiceOptionParam</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.chat_completion_function_call_option_param</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ChatCompletionFunctionCallOptionParam</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;CompletionCreateParamsBase&quot;</span><span class="p">,</span>
    <span class="s2">&quot;FunctionCall&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Function&quot;</span><span class="p">,</span>
    <span class="s2">&quot;ResponseFormat&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CompletionCreateParamsNonStreaming&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CompletionCreateParamsStreaming&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="CompletionCreateParamsBase">
<a class="viewcode-back" href="../../../../openai.types.chat.html#openai.types.chat.completion_create_params.CompletionCreateParamsBase">[docs]</a>
<span class="k">class</span> <span class="nc">CompletionCreateParamsBase</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">messages</span><span class="p">:</span> <span class="n">Required</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">ChatCompletionMessageParam</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A list of messages comprising the conversation so far.</span>

<span class="sd">    [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">model</span><span class="p">:</span> <span class="n">Required</span><span class="p">[</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="nb">str</span><span class="p">,</span>
            <span class="n">Literal</span><span class="p">[</span>
                <span class="s2">&quot;gpt-4-1106-preview&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-vision-preview&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-0314&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-0613&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-32k&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-32k-0314&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-4-32k-0613&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo-16k&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo-0301&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo-0613&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo-1106&quot;</span><span class="p">,</span>
                <span class="s2">&quot;gpt-3.5-turbo-16k-0613&quot;</span><span class="p">,</span>
            <span class="p">],</span>
        <span class="p">]</span>
    <span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ID of the model to use.</span>

<span class="sd">    See the</span>
<span class="sd">    [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility)</span>
<span class="sd">    table for details on which models work with the Chat API.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">frequency_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number between -2.0 and 2.0.</span>

<span class="sd">    Positive values penalize new tokens based on their existing frequency in the</span>
<span class="sd">    text so far, decreasing the model&#39;s likelihood to repeat the same line verbatim.</span>

<span class="sd">    [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">function_call</span><span class="p">:</span> <span class="n">FunctionCall</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deprecated in favor of `tool_choice`.</span>

<span class="sd">    Controls which (if any) function is called by the model. `none` means the model</span>
<span class="sd">    will not call a function and instead generates a message. `auto` means the model</span>
<span class="sd">    can pick between generating a message or calling a function. Specifying a</span>
<span class="sd">    particular function via `{&quot;name&quot;: &quot;my_function&quot;}` forces the model to call that</span>
<span class="sd">    function.</span>

<span class="sd">    `none` is the default when no functions are present. `auto` is the default if</span>
<span class="sd">    functions are present.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">functions</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Function</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Deprecated in favor of `tools`.</span>

<span class="sd">    A list of functions the model may generate JSON inputs for.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">logit_bias</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Modify the likelihood of specified tokens appearing in the completion.</span>

<span class="sd">    Accepts a JSON object that maps tokens (specified by their token ID in the</span>
<span class="sd">    tokenizer) to an associated bias value from -100 to 100. Mathematically, the</span>
<span class="sd">    bias is added to the logits generated by the model prior to sampling. The exact</span>
<span class="sd">    effect will vary per model, but values between -1 and 1 should decrease or</span>
<span class="sd">    increase likelihood of selection; values like -100 or 100 should result in a ban</span>
<span class="sd">    or exclusive selection of the relevant token.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Whether to return log probabilities of the output tokens or not.</span>

<span class="sd">    If true, returns the log probabilities of each output token returned in the</span>
<span class="sd">    `content` of `message`. This option is currently not available on the</span>
<span class="sd">    `gpt-4-vision-preview` model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">max_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The maximum number of [tokens](/tokenizer) that can be generated in the chat</span>
<span class="sd">    completion.</span>

<span class="sd">    The total length of input tokens and generated tokens is limited by the model&#39;s</span>
<span class="sd">    context length.</span>
<span class="sd">    [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)</span>
<span class="sd">    for counting tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">n</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;How many chat completion choices to generate for each input message.</span>

<span class="sd">    Note that you will be charged based on the number of generated tokens across all</span>
<span class="sd">    of the choices. Keep `n` as `1` to minimize costs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">presence_penalty</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Number between -2.0 and 2.0.</span>

<span class="sd">    Positive values penalize new tokens based on whether they appear in the text so</span>
<span class="sd">    far, increasing the model&#39;s likelihood to talk about new topics.</span>

<span class="sd">    [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">response_format</span><span class="p">:</span> <span class="n">ResponseFormat</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An object specifying the format that the model must output.</span>

<span class="sd">    Compatible with `gpt-4-1106-preview` and `gpt-3.5-turbo-1106`.</span>

<span class="sd">    Setting to `{ &quot;type&quot;: &quot;json_object&quot; }` enables JSON mode, which guarantees the</span>
<span class="sd">    message the model generates is valid JSON.</span>

<span class="sd">    **Important:** when using JSON mode, you **must** also instruct the model to</span>
<span class="sd">    produce JSON yourself via a system or user message. Without this, the model may</span>
<span class="sd">    generate an unending stream of whitespace until the generation reaches the token</span>
<span class="sd">    limit, resulting in a long-running and seemingly &quot;stuck&quot; request. Also note that</span>
<span class="sd">    the message content may be partially cut off if `finish_reason=&quot;length&quot;`, which</span>
<span class="sd">    indicates the generation exceeded `max_tokens` or the conversation exceeded the</span>
<span class="sd">    max context length.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This feature is in Beta. If specified, our system will make a best effort to</span>
<span class="sd">    sample deterministically, such that repeated requests with the same `seed` and</span>
<span class="sd">    parameters should return the same result. Determinism is not guaranteed, and you</span>
<span class="sd">    should refer to the `system_fingerprint` response parameter to monitor changes</span>
<span class="sd">    in the backend.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">stop</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Up to 4 sequences where the API will stop generating further tokens.&quot;&quot;&quot;</span>

    <span class="n">temperature</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;What sampling temperature to use, between 0 and 2.</span>

<span class="sd">    Higher values like 0.8 will make the output more random, while lower values like</span>
<span class="sd">    0.2 will make it more focused and deterministic.</span>

<span class="sd">    We generally recommend altering this or `top_p` but not both.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tool_choice</span><span class="p">:</span> <span class="n">ChatCompletionToolChoiceOptionParam</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Controls which (if any) function is called by the model. `none` means the model</span>
<span class="sd">    will not call a function and instead generates a message. `auto` means the model</span>
<span class="sd">    can pick between generating a message or calling a function. Specifying a</span>
<span class="sd">    particular function via</span>
<span class="sd">    `{&quot;type: &quot;function&quot;, &quot;function&quot;: {&quot;name&quot;: &quot;my_function&quot;}}` forces the model to</span>
<span class="sd">    call that function.</span>

<span class="sd">    `none` is the default when no functions are present. `auto` is the default if</span>
<span class="sd">    functions are present.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">tools</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">ChatCompletionToolParam</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A list of tools the model may call.</span>

<span class="sd">    Currently, only functions are supported as a tool. Use this to provide a list of</span>
<span class="sd">    functions the model may generate JSON inputs for.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">top_logprobs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An integer between 0 and 5 specifying the number of most likely tokens to return</span>
<span class="sd">    at each token position, each with an associated log probability. `logprobs` must</span>
<span class="sd">    be set to `true` if this parameter is used.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">top_p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An alternative to sampling with temperature, called nucleus sampling, where the</span>
<span class="sd">    model considers the results of the tokens with top_p probability mass. So 0.1</span>
<span class="sd">    means only the tokens comprising the top 10% probability mass are considered.</span>

<span class="sd">    We generally recommend altering this or `temperature` but not both.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">user</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A unique identifier representing your end-user, which can help OpenAI to monitor</span>
<span class="sd">    and detect abuse.</span>
<span class="sd">    [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).</span>
<span class="sd">    &quot;&quot;&quot;</span></div>



<span class="n">FunctionCall</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="s2">&quot;auto&quot;</span><span class="p">],</span> <span class="n">ChatCompletionFunctionCallOptionParam</span><span class="p">]</span>


<div class="viewcode-block" id="Function">
<a class="viewcode-back" href="../../../../openai.types.chat.html#openai.types.chat.completion_create_params.Function">[docs]</a>
<span class="k">class</span> <span class="nc">Function</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Required</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The name of the function to be called.</span>

<span class="sd">    Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length</span>
<span class="sd">    of 64.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">description</span><span class="p">:</span> <span class="nb">str</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A description of what the function does, used by the model to choose when and</span>
<span class="sd">    how to call the function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">parameters</span><span class="p">:</span> <span class="n">shared_params</span><span class="o">.</span><span class="n">FunctionParameters</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The parameters the functions accepts, described as a JSON Schema object.</span>

<span class="sd">    See the</span>
<span class="sd">    [guide](https://platform.openai.com/docs/guides/text-generation/function-calling)</span>
<span class="sd">    for examples, and the</span>
<span class="sd">    [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for</span>
<span class="sd">    documentation about the format.</span>

<span class="sd">    Omitting `parameters` defines a function with an empty parameter list.</span>
<span class="sd">    &quot;&quot;&quot;</span></div>



<div class="viewcode-block" id="ResponseFormat">
<a class="viewcode-back" href="../../../../openai.types.chat.html#openai.types.chat.completion_create_params.ResponseFormat">[docs]</a>
<span class="k">class</span> <span class="nc">ResponseFormat</span><span class="p">(</span><span class="n">TypedDict</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="nb">type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;json_object&quot;</span><span class="p">]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Must be one of `text` or `json_object`.&quot;&quot;&quot;</span></div>



<div class="viewcode-block" id="CompletionCreateParamsNonStreaming">
<a class="viewcode-back" href="../../../../openai.types.chat.html#openai.types.chat.completion_create_params.CompletionCreateParamsNonStreaming">[docs]</a>
<span class="k">class</span> <span class="nc">CompletionCreateParamsNonStreaming</span><span class="p">(</span><span class="n">CompletionCreateParamsBase</span><span class="p">):</span>
    <span class="n">stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="kc">False</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;If set, partial message deltas will be sent, like in ChatGPT.</span>

<span class="sd">    Tokens will be sent as data-only</span>
<span class="sd">    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)</span>
<span class="sd">    as they become available, with the stream terminated by a `data: [DONE]`</span>
<span class="sd">    message.</span>
<span class="sd">    [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).</span>
<span class="sd">    &quot;&quot;&quot;</span></div>



<div class="viewcode-block" id="CompletionCreateParamsStreaming">
<a class="viewcode-back" href="../../../../openai.types.chat.html#openai.types.chat.completion_create_params.CompletionCreateParamsStreaming">[docs]</a>
<span class="k">class</span> <span class="nc">CompletionCreateParamsStreaming</span><span class="p">(</span><span class="n">CompletionCreateParamsBase</span><span class="p">):</span>
    <span class="n">stream</span><span class="p">:</span> <span class="n">Required</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="kc">True</span><span class="p">]]</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;If set, partial message deltas will be sent, like in ChatGPT.</span>

<span class="sd">    Tokens will be sent as data-only</span>
<span class="sd">    [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)</span>
<span class="sd">    as they become available, with the stream terminated by a `data: [DONE]`</span>
<span class="sd">    message.</span>
<span class="sd">    [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).</span>
<span class="sd">    &quot;&quot;&quot;</span></div>



<span class="n">CompletionCreateParams</span> <span class="o">=</span> <span class="n">Union</span><span class="p">[</span><span class="n">CompletionCreateParamsNonStreaming</span><span class="p">,</span> <span class="n">CompletionCreateParamsStreaming</span><span class="p">]</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, openai llc.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>